In retrospect, Amdahl's Law states the obvious, i.e. the amount of speedup that can be achieved through parallelism is limited by the portion of the program that cannot be parallelized. Clearly, this vindicates the belief that the program is only as good as the algorithm. Therefore it is important to capture the intents of an algorithm in a program that fully exploits the underlying processing platform. Several programming languages approach the problem of expressing a parallel algorithm via supporting multithreading, multiprocessing etc. Programming models such as OpenMP and MPI provide extensions to existing languages to support parallel patterns and arbitrary task graphs. PGAS programming languages such as X10\ref{}, Chapel \ref{} ease the programmers' burden by offering language constructs that spawn tasks and manage inter-task synchronization, also allowing the programmer to distribute data onto physically distributed memory with a  shared-address space. 