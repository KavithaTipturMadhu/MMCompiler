In order to achieve optimal performance, it is important to capture the intents of an algorithm in a program that fully exploits the underlying processing platform. Several programming languages approach the problem of expressing parallel algorithms via support for multithreading, multiprocessing etc. Programming models such as OpenMP and MPI provide extensions to existing languages to support parallel patterns, the newer versions of which support arbitrary task graphs (OpenMP 3.0\ref{}). PGAS programming languages such as X10\ref{}, Chapel \ref{} ease the programmers' burden by offering language constructs that spawn tasks and manage inter-task synchronization, also allowing the programmer to distribute data onto physically distributed memory with a  shared address space. Compiling such languages onto the same target runtime can be a challenge, as is reusing high level optimizations due to custom compilation toolchains. There hence arises the need for a parallel IR which allows compiling various applications easily onto the same target, while enabling common optimizations among them. CnC is one such language designed to increase programmability that separates the concerns of expressing parallelism and tuning it to specific targets. Further, DFGR was proposed as an intermediate representation that assumes data race free execution of tasks. It extends CnC by supporting direct predication of a task by another and allowing communication of aggregate data objects such as ranges and regions between tasks. 

In this work, we present an intermediate representation based on DFGR implemented as an extension of LLVM IR which eases static analysis and optimizations of applications at a macro level. The proposed IR captures the dynamic behavior of tasks that compose the application and further ensures deadlock freedom in dependences across tasks, as will be explained subsequently. The IR also comprises a subset of tuning directives to distribute input and output data onto various chunks of memory by allowing domain map definitions. Further, data parallelism is enabled exposing the underlying hardware resources as locales, an idea borrowed from Chapel programming language, similarly as \textit{places} in X10. A subset of regularly occurring parallel patterns and data distribution libraries are provided that maybe enumerated into native graph description, enabling the compiler to perform target specific optimizations, such as offering locality in activation frames. 