\label{sec:llvm_ir}
LLVM IR generates code using its virtual instruction set targeting unicore processors, making it unsuitable for representing parallel programs. State of the art parallel frontends are usually designed to suit the underlying hardware model (shared memory multithreading model assumed by OpenMP vs distributed memory model in case of MPI). A target agnostic representation of parallel programs based on DFGR\cite{} is presented in this work that describes kernels in a graph-like manner. An application kernel described using DFGR comprises compute steps that represent scheduleable computations and item collections that represent data communicated between them. Compute step instances maybe spawned by other compute steps and the creator-created, controller-controllee relationships are captured in DFGR. This representation aims at easing the process of developing a new frontend for any parallel hardware target so long as there exists runtime support to execute the kernel on the target. DFGR also encompasses dynamic behavior of applications unlike the dataflow graph representation.

We propose realization of DFGR (in the current LLVM framework) by a parallel IR whose description is as follows.
\subsection{Representing Computation}
Compute steps of DFGR are represented as functions in LLVM IR. Statically enumerated compute steps are uniquely identified by the function's signature as the identifier. Dynamic instances of compute steps are identified by the function signature along with a tag, akin to the DFGR compute step tags. The tag is used to derive producer-consumer relationship between compute steps in our work and does not see a direct realization at runtime. To ease static analysis, we use limited set of functions defined on tags described next to define relationships between dynamic compute step instances.
\begin{itemize}
 \item $<id>$ is a self-referential tag of a compute step.
 \item $<suffix(id,i)>$ represents a step with suffix \textit{i} of the self referential id. 
 \item $<prefix(id,n)>$ represents the prefix of an id obtained by considering the first n entries of the self-referential id. Further, nesting of prefix function is allowed.
\end{itemize}

We observe that the aforementioned functions suffice for representing any kernel in our IR with the assumption that runtime manages instances independent of tag functions. Arbitrary tag functions make static analysis difficult but maybe easier to program. Hence, we support linear tag relationships to ease the programmer's burden. Any linear transformation of the form $a\times id + b $ is supported, where $a$ and $b$ are scalars and so is further nesting of tag functions (for example, transformation of the form $a\times prefix(id, i) + b $).

\subsection{Representing Communication}
Communication of data between compute steps of the parallel IR is represented using custom named metadata in LLVM IR that labels data with its consumer(s). Approaches proposed in \cite{} realize communication via intrinsic extensions to LLVM and assume software support for communication. On the other hand, our approach makes no assumptions about the mode of realizing communication and hence can be compiled to esoteric targets(ASIC, FPGA) as well without performing machine level optimizations of removing communication instructions. The parallel IR defined as per the specification listed above can be compiled suitably onto any target and is open to local as well as graph optimizations and modifies the source IR to the least extent possible. 

Communication in our parallel IR is asynchronous and the consumer is unaware of the source of received data. Communication between compute steps i.e., HyperOps are of three types namely \textit{data, control, synchronization}. Interim data produced by a HyperOp (represented as a function) is annotated to define who the consumers are and what the data is used for (i.e., whether \textit{ConsumedBy} or \textit{Controls} or \textit{SyncWith} consumer HyperOp(s)). In order to offer the flexibility of either using a single copy of the data that gets consumed by other HyperOps or to follow dynamic single assignment rule, data is not associated with instance ids (tags). This deviates slightly from DFGR where data is associated with instance ids uniformly. DFGR also assumes that compute steps are side-effect free. In our case, compute step instances that modify the global state are explicitly serialized by the compiler. Further, the data being communicated is marked as \textit{static} or \textit{composite} in order to assist the compiler in mapping data onto different modes of communication efficiently. Additionally, in case of the other two forms of communication i.e., \textit{control} and \textit{synchronization}, the runtime is free to realize data-based synchronization and control flow (like CnC) or may choose to support explicit control and synchronization primitives. The consumer HyperOps, when statically enumerated, are listed with the metadata linked with the data. A range of consumers may also be specified by extending the suffix function (described in the previous section) with vector extensions. Recall that $<suffix(id,i)>$ was earlier defined over scalar ids. These functions are extended to to support vector inputs in place of $i$ which can now take a range of integer values specified as $[m:n]$, including $m$ and $n$. $n$ maybe variable but the lower bound $m$ needs to be known. As mentioned previously, linear transformation defined over a range may also be used as the vector input to suffix function.

\textbf{TODO: Topics of discussion here:\\
Disadvantages of the current IR implementation: Traditional optimizations may not be easily performed on the IR since metadata is ignored during compiler optimizations. CnC defines producer-consumer relationships at the boundaries of HyperOps and their compiler generates skeleton code for the programmer to fill with \textit{get} and \textit{put} calls assuming item collections to exist. This allows the communication to have a memory footprint and hence allows optimizations to go on since the memory operations change the global state. However, this hinders graph optimizations such as locality optimizations. Another approach proposed for PGAS languages on LLVM IR states that a shared address space in LLVM maybe written into or read from. I need to verify if a large number of address spaces can be created in LLVM and if they can be parametrically created and addressed. If this is indeed available, a pass that converts communication to writes and reads to address spaces can be implemented that allows standard optimizations to proceed. Another pass that eliminates these instructions and reintroduces the metadata maybe implemented. If feasible, this should be an ideal solution to our problems.}